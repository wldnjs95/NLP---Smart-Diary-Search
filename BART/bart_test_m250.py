# -*- coding: utf-8 -*-
"""bart_test_m250.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1le7hFVKK3x1P4ocu_9KpyM1DPUY1Y59M
"""

!pip install transformers datasets torch
!pip install datasets
!pip install transformers datasets pandas scikit-learn
!pip install --upgrade gradio
!pip install sentence-transformers

import json
import torch
from transformers import BartTokenizer, BartForConditionalGeneration, AdamW

#load JSON data
json_file_path = "m-250.json"
with open(json_file_path, "r") as file:
    diary_data_direct = json.load(file)

def prepare_data(data):
    input_texts = []
    target_texts = []

    for diary_id, diary_content in data.items():
        diary_entry = diary_content["diary_entry"]
        keywords = diary_content["extracted_keywords"]

        #input prompt
        input_text = f"Extract all Events, Actions, Times, and Thoughts from: '{diary_entry}'"

        #target output: Convert extracted keywords to a delimited format
        event_text = "; ".join(keywords.get("event", []))
        action_text = "; ".join(keywords.get("action", []))
        time_text = "; ".join(keywords.get("time", []))
        thought_text = "; ".join(keywords.get("thoughts", []))

        #format the target text
        target_text = f"Events: {event_text} | Actions: {action_text} | Times: {time_text} | Thoughts: {thought_text}"

        input_texts.append(input_text)
        target_texts.append(target_text)

    return input_texts, target_texts


#prepare inputs and outputs
input_texts, target_texts = prepare_data(diary_data_direct)

#load tokenizer and model
model_name = "facebook/bart-base"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

#tokenization for Seq2Seq
def tokenize_data(input_texts, target_texts, tokenizer):
    inputs = tokenizer(input_texts, max_length=512, truncation=True, padding="max_length", return_tensors="pt")
    targets = tokenizer(target_texts, max_length=512, truncation=True, padding="max_length", return_tensors="pt")
    return inputs, targets

inputs, targets = tokenize_data(input_texts, target_texts, tokenizer)

#optimizer
optimizer = AdamW(model.parameters(), lr=1e-5)

#fine-tuning loop
num_epochs = 3
model.train()

for epoch in range(num_epochs):
    total_loss = 0

    for i in range(len(inputs["input_ids"])):
        input_ids = inputs["input_ids"][i].unsqueeze(0)
        attention_mask = inputs["attention_mask"][i].unsqueeze(0)
        labels = targets["input_ids"][i].unsqueeze(0)

        #forward pass
        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        #backward pass
        loss.backward()
        optimizer.step()

    avg_loss = total_loss / len(inputs["input_ids"])
    print(f"Epoch {epoch + 1} Average Loss: {avg_loss}")

#save the model
save_directory = "fine_tuned_bart_pt2"
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)
print(f"Model saved to {save_directory}")

#testing the model
model.eval()

def test_model(diary_entry):
    input_text = f"Extract all Events, Actions, Times, and Thoughts from: '{diary_entry}'"
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)

    with torch.no_grad():
        generated_ids = model.generate(
            inputs["input_ids"],
            max_length=256,
            num_beams=4,
            early_stopping=True
        )
        output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    results = {}
    for part in output.split("|"):
        key, values = part.split(": ", 1)
        results[key.strip().lower()] = [v.strip() for v in values.split(";")]
    return results


#test entries - t5 testing before
test_entries = [
    "I woke up to the gentle sound of dripping water. The air was heavy with humidity, clinging to my skin. The house felt unusually quiet, as if holding its breath. Stepping outside, I noticed puddles forming under a gray sky. A light rain had begun, soft and soothing, bringing a sense of calm. It felt like the world was pausing, just for a moment.",
    "The party started at 4 PM, and by midnight, it still showed no signs of ending. Laughter and music filled the air as plates piled up and drinks kept flowing. I watched as conversations turned deeper, and some danced like no one was watching. Though I felt tired, I couldn’t help but smile—it’s rare to experience such a lively night. The clock ticked past midnight, but no one seemed to care.",
    "The flight to London, once $600, skyrocketed to $3600 in just four days. I stared at the screen in disbelief, refreshing the page as if hoping it was a glitch. Planning the trip felt exciting at first, but now it seemed almost impossible. I wondered if I should wait, gamble on a price drop, or let the dream go for now. Travel, it seems, has its own way of testing patience."
]

#generate predictions for test entries
for idx, entry in enumerate(test_entries):
    print(f"Test Entry {idx + 1}: {entry}")
    predictions = test_model(entry)
    print(f"Predicted Output {idx + 1}: {predictions}\n")

import pandas as pd
import torch
import json
from transformers import BartForConditionalGeneration, BartTokenizer

#load the fine-tuned model and tokenizer
model_name = "fine_tuned_bart_pt2"
model = BartForConditionalGeneration.from_pretrained(model_name)
tokenizer = BartTokenizer.from_pretrained(model_name)

#load the test dataset
file_path = "test-diary-30-fm.csv"
test_data = pd.read_csv(file_path)

#extract diary entries
texts = test_data['diary_entry'].tolist()

#task-specific prompt - same as model fine tune
def test_model(diary_entry):
    input_text = f"Extract all Events, Actions, Times, and Thoughts from: '{diary_entry}'"
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)

    with torch.no_grad():
        generated_ids = model.generate(
            inputs["input_ids"],
            max_length=256,
            num_beams=4,
            early_stopping=True
        )
        output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    return output

#generate predictions for CSV data
structured_results = []
for text in texts:
    prediction = test_model(text)
    print(f"Raw Prediction: {prediction}")

    try:
        structured_output = json.loads(prediction)
    except json.JSONDecodeError:
        structured_output = {}
        for part in prediction.split("|"):
            if ": " in part:
                key, value = part.split(": ", 1)
                structured_output[key.strip().lower()] = value.strip()

    #default to None for missing keys
    structured_results.append({
        "text": text,
        "predicted": {
            "event": structured_output.get("events", None),
            "action": structured_output.get("actions", None),
            "time": structured_output.get("times", None),
            "thought": structured_output.get("thoughts", None)
        }
    })

#results
for result in structured_results:
    print(f"Input Text: {result['text']}")
    print("NER Prediction:")
    for key, value in result["predicted"].items():
        print(f"  {key}: {value}")
    print("-" * 50)

#save results to a CSV file
output_df = pd.DataFrame(structured_results)
output_df.to_csv("predicted_results.csv", index=False)
print("Results saved to 'predicted_results.csv'.")

"""### debugging // weird characters"""

import pandas as pd
import torch
import json
from transformers import BartForConditionalGeneration, BartTokenizer
import unicodedata

#load the fine-tuned model and tokenizer
model_name = "fine_tuned_bart_pt2"
model = BartForConditionalGeneration.from_pretrained(model_name)
tokenizer = BartTokenizer.from_pretrained(model_name)

#load the test dataset
file_path = "test-diary-30-fm.csv"
test_data = pd.read_csv(file_path)

#extract diary entries
texts = test_data['diary_entry'].tolist()

#function to clean text encoding issues
def clean_text(text):
    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')

#task-specific prompt
def test_model(diary_entry):
    input_text = f"Extract all Events, Actions, Times, and Thoughts from: '{diary_entry}'"
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)

    with torch.no_grad():
        generated_ids = model.generate(
            inputs["input_ids"],
            max_length=256,
            num_beams=4,
            early_stopping=True
        )
        output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    return output

#generate predictions for CSV data
structured_results = []
for text in texts:
    prediction = test_model(clean_text(text))
    print(f"Raw Prediction: {prediction}")

    try:
        structured_output = json.loads(prediction)
    except json.JSONDecodeError:
        structured_output = {}
        for part in prediction.split("|"):
            if ": " in part:
                key, value = part.split(": ", 1)
                structured_output[key.strip().lower()] = value.strip()

    #default to None for missing keys and clean text outputs
    structured_results.append({
        "diary_entry": clean_text(text),  # Clean the diary entry text
        "event": clean_text(structured_output.get("events", "None")),
        "action": clean_text(structured_output.get("actions", "None")),
        "time": clean_text(structured_output.get("times", "None")),
        "thought": clean_text(structured_output.get("thoughts", "None"))
    })

#convert results to a DataFrame
results_df = pd.DataFrame(structured_results)

#save the results to a CSV file
output_file_path = "predicted_diary_results_cleaned.csv"
results_df.to_csv(output_file_path, index=False)
print(f"Predicted results saved to '{output_file_path}'.")

#test entries - using training data
test_train_entries = [
    "This morning was life-changing. I took a walk on the beach, with the waves crashing like a soundtrack. Then, I stopped for a black americano, the most exciting drink on the menu, and stared out the window in awe of life’s mundanity. The highlight of my day? The peacefulness. I’ll spend the rest of the day recovering from this thrilling routine. Who knew mornings could be this uneventful yet somehow fulfilling?",
    "Tonight was monumental—I watched my favorite movie again on Netflix, even though I know every line. It’s comforting to watch characters make the same mistakes. The suspense? Gone. But hey, I still recommend it to my friends like it’s some hidden gem. The emotional impact? Practically non-existent. I’ll probably watch it again tomorrow. It’s the cinematic revelation of my week, truly a comforting routine.",
    "I spent my afternoon reading, the same thrill as watching paint dry, followed by celebrating my friend Meghna’s 21st birthday at a trendy restaurant. We oohed and aahed over her ‘first legal drink’ like it was a monumental occasion. We laughed and pretended 21 was a magical age. It was a celebration of pretend milestones, making it seem like life somehow changed overnight."
]

#generate predictions for test entries
for idx, entry in enumerate(test_train_entries):
    print(f"Test Entry {idx + 1}: {entry}")
    predictions = test_model(entry)
    print(f"Predicted Output {idx + 1}: {predictions}\n")

"""## run with 30 epoch // save every epoch"""

import pandas as pd
import torch
import json
from transformers import BartForConditionalGeneration, BartTokenizer, AdamW
import unicodedata
import os

# Load the fine-tuned model and tokenizer
model_name = "facebook/bart-base"  # Start from pre-trained base model
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# Load the training dataset
train_file_path = "m-250.json"
with open(train_file_path, "r") as file:
    train_data = json.load(file)

# Load the test dataset
test_file_path = "test-diary-30-fm.csv"
test_data = pd.read_csv(test_file_path)

# Extract diary entries and prepare data
def prepare_data(data):
    input_texts = []
    target_texts = []

    for diary_id, diary_content in data.items():
        diary_entry = diary_content["diary_entry"]
        keywords = diary_content["extracted_keywords"]

        input_text = f"Extract all Events, Actions, Times, and Thoughts from: '{diary_entry}'"
        target_text = json.dumps(keywords)
        input_texts.append(input_text)
        target_texts.append(target_text)

    return input_texts, target_texts

train_inputs, train_targets = prepare_data(train_data)

# Tokenize data
def tokenize_data(input_texts, target_texts, tokenizer):
    inputs = tokenizer(input_texts, max_length=512, truncation=True, padding="max_length", return_tensors="pt")
    targets = tokenizer(target_texts, max_length=512, truncation=True, padding="max_length", return_tensors="pt")
    return inputs, targets

train_inputs, train_targets = tokenize_data(train_inputs, train_targets, tokenizer)

# Split data into training and validation
validation_split = 0.1
num_validation = int(len(train_inputs["input_ids"]) * validation_split)
train_indices = list(range(len(train_inputs["input_ids"]) - num_validation))
val_indices = list(range(len(train_inputs["input_ids"]) - num_validation, len(train_inputs["input_ids"])))

# Create validation and training sets
val_inputs = {k: v[val_indices] for k, v in train_inputs.items()}
val_targets = {k: v[val_indices] for k, v in train_targets.items()}
train_inputs = {k: v[train_indices] for k, v in train_inputs.items()}
train_targets = {k: v[train_indices] for k, v in train_targets.items()}

# Optimizer
optimizer = AdamW(model.parameters(), lr=1e-5)

# Clean text encoding issues
def clean_text(text):
    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')

# Validation function
def evaluate_model(val_inputs, val_targets, model):
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for i in range(len(val_inputs["input_ids"])):
            input_ids = val_inputs["input_ids"][i].unsqueeze(0)  # Add batch dimension
            attention_mask = val_inputs["attention_mask"][i].unsqueeze(0)
            labels = val_targets["input_ids"][i].unsqueeze(0)  # Add batch dimension
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            val_loss += outputs.loss.item()
    model.train()
    return val_loss / len(val_inputs["input_ids"])

# Fine-tuning loop with epoch-wise results and early stopping
num_epochs = 20
patience = 5  # Number of epochs to wait before stopping if no improvement
best_val_loss = float("inf")
early_stop_counter = 0

save_directory = "saved_models"
os.makedirs(save_directory, exist_ok=True)

for epoch in range(num_epochs):
    total_loss = 0

    # Training
    model.train()
    for i in range(len(train_inputs["input_ids"])):
        input_ids = train_inputs["input_ids"][i].unsqueeze(0)  # Add batch dimension
        attention_mask = train_inputs["attention_mask"][i].unsqueeze(0)
        labels = train_targets["input_ids"][i].unsqueeze(0)  # Add batch dimension

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

    avg_loss = total_loss / len(train_inputs["input_ids"])

    # Validation
    val_loss = evaluate_model(val_inputs, val_targets, model)
    print(f"Epoch {epoch + 1} - Training Loss: {avg_loss}, Validation Loss: {val_loss}")

    # Save model for this epoch
    model_save_path = os.path.join(save_directory, f"model_epoch_{epoch + 1}.pt")
    torch.save(model.state_dict(), model_save_path)
    print(f"Model saved at {model_save_path}")

    # Early stopping logic
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        early_stop_counter = 0
    else:
        early_stop_counter += 1
        if early_stop_counter >= patience:
            print(f"Early stopping triggered after {epoch + 1} epochs.")
            break

# Final evaluation on 30 test samples
print("\n--- Final Test Results ---")
model.eval()
results = []
for diary_entry in test_data["diary_entry"].tolist():
    inputs = tokenizer(diary_entry, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        generated_ids = model.generate(
            inputs["input_ids"],
            max_length=256,
            num_beams=4,
            early_stopping=True
        )
    prediction = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    results.append({
        "diary_entry": clean_text(diary_entry),
        "prediction": clean_text(prediction)
    })

# Save results to CSV
output_df = pd.DataFrame(results)
output_df.to_csv("final_test_results_cleaned.csv", index=False)
print("Final test results saved to 'final_test_results_mcleaned.csv'.")